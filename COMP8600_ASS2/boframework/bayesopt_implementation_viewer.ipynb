{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression and Bayesian Global Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import gp as gp\n",
    "import kernels as kernels\n",
    "import acquisitions as acquisitions \n",
    "import bayesopt as bayesopt\n",
    "import importlib\n",
    "\n",
    "importlib.reload(gp)\n",
    "importlib.reload(kernels)\n",
    "importlib.reload(acquisitions)\n",
    "importlib.reload(bayesopt)\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['xtick.major.pad']='2'\n",
    "plt.rcParams['ytick.major.pad']='2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to maximise the following 1-dimensional function,\n",
    "$$\\mathrm{maximise}_x\\quad f(x)$$\n",
    "where\n",
    "$$f(x) = \\sin(x) + \\sin(2x) + \\epsilon$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = np.array([[-3, 6]])\n",
    "noise_level = 0.1\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "def f(X, noise_level=noise_level):\n",
    "    return np.sin(X) + np.sin(2 * X) + noise_level * np.random.randn(*X.shape)\n",
    "\n",
    "X_init = np.array([[-0.5], [2.2]])\n",
    "Y_init = f(X_init)\n",
    "\n",
    "# Bound our random variable X\n",
    "X = np.arange(bounds[:, 0], bounds[:, 1], 0.02).reshape(-1, 1)\n",
    "\n",
    "# First let's have a noise-free objective function\n",
    "Y = f(X, 0)\n",
    "\n",
    "#Plot optimisation objective with the appropriate noise levels\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(X, Y, 'r--', lw=2, label='Noise-free objective');\n",
    "plt.plot(X, f(X), 'bx', lw=1, alpha=0.6, label='Noisy samples');\n",
    "plt.plot(X_init, Y_init, 'kx', mew=3, label='Initial samples');\n",
    "plt.title(\"Expensive Black-Box Function\", fontdict = {'fontsize' : 20})\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Output:\n",
    "# 1) An animation of the Bayesian optimisation process which will lead to the close approximation of the surrogate function to the black box one.\n",
    "# 2) An image of the final iteration (result) of the BO process (expected to be at a converged state)\n",
    "# 3) Two plots indicating extra convergence heuristics\n",
    "\n",
    "# Probability Improvement (PI)\n",
    "bayesopt.BO(X_init, Y_init, f, noise_level, bounds, n_iter=12, xi=0.2, title='Bayesian Optimisation - with Probability Improvement', X=X, Y=Y).call_animation(acquisitions.probability_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Output: Similar as above\n",
    "\n",
    "# Expected Improvement (EI)\n",
    "bayesopt.BO(X_init, Y_init, f, noise_level, bounds, n_iter=12, xi=0.05, title='Bayesian Optimisation - with Expected Improvement', X=X, Y=Y).call_animation(acquisitions.expected_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher Dimensions\n",
    "\n",
    "Let's assume that we want to approximate a higher dimensional function via the surrogate model, using a Gaussian Process regression.\n",
    "\n",
    "We have the function of a N-dimensional dataset $X$, \n",
    "$$\n",
    "f(X) = \\tanh^2\\left(\\frac{||X||_F}{2}\\right) + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "where we take the Frobenius norm of $X$ per column (second dimension).\n",
    "\n",
    "Let's now investigate the benefits of performing hyper-parameter optimisation on the Gaussian process regression model during the fitting process by maximising the log marginal likelihood, before performing tasks such as Bayesian optimisation to find global optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp_2D(gx, gy, mu, X_train, Y_train, title, i):\n",
    "    ax = plt.gcf().add_subplot(1, 3, i, projection='3d')\n",
    "    ax.plot_surface(gx, gy, mu.reshape(gx.shape), cmap=cm.winter, linewidth=0.5, alpha=0.3, antialiased=False)\n",
    "    ax.scatter(X_train[:,0], X_train[0:,1], Y_train, c=Y_train, cmap=cm.autumn)\n",
    "    ax.set_title(title)\n",
    "\n",
    "noise_2D = 0.1\n",
    "np.random.seed(3408944656)\n",
    "\n",
    "def f2D(X, noise_2D=noise_2D):\n",
    "    return np.tanh(0.5 * np.linalg.norm(X, axis=1))**2 + noise_2D * np.random.randn(len(X))\n",
    "\n",
    "\n",
    "rx, ry = np.arange(-10, 10, 0.1), np.arange(-10, 10, 0.1)\n",
    "gx, gy = np.meshgrid(rx, rx)\n",
    "X_2D = np.c_[gx.ravel(), gy.ravel()]\n",
    "\n",
    "Y_1D = f2D(X_2D, 0)\n",
    "\n",
    "X_2D_train = np.random.uniform(-8, 8, (100, 2))\n",
    "\n",
    "Y_1D_train = f2D(X_2D_train)\n",
    "\n",
    "# TODO Q2.8b\n",
    "#-------------------------------------------------------------\n",
    "# FIXME\n",
    "nu_s = []\n",
    "#------------------------------------------------------------\n",
    "\n",
    "for nu in nu_s:\n",
    "\n",
    "    m = kernels.Matern(nu=nu, length_scale=1.0, variance=1.0)\n",
    "    gpr = gp.GPR(kernel=m, noise_level=noise_2D**2, n_restarts=20)\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    gpr.X_train = X_2D_train\n",
    "    gpr.y_train = Y_1D_train\n",
    "    mu_s = gpr.predict(X_2D)\n",
    "    plot_gp_2D(gx, gy, mu_s, X_2D_train, Y_1D_train, r'Before hyper-parameter optimisation: $\\ell$=1.00, $\\sigma^2_f$=1.00', 1)\n",
    "\n",
    "    gpr.update(X_2D_train, Y_1D_train)\n",
    "    params = gpr.kernel.get_hyperparameters()\n",
    "    mu_s = gpr.predict(X_2D)\n",
    "    print(f'Running a Gaussian process with a Matern class covariance function with value nu = {nu}')\n",
    "    for key in sorted(params): print(f\"{key} : {params[key]}\")\n",
    "\n",
    "    plot_gp_2D(gx, gy, mu_s, X_2D_train, Y_1D_train,\n",
    "            r'After hyper-parameter optimisation: $\\ell$={:.2f}, $\\sigma^2_f$={:.2f}'.format(params[\"length_scale\"], params[\"variance\"]), 2)\n",
    "\n",
    "    plot_gp_2D(gx, gy, Y_1D, X_2D_train, Y_1D_train, f'Original function without noise', 3)\n",
    "    plt.show()\n",
    "    print('-------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "# Expected output per loop/row:\n",
    "# Three 3D plots: GP approximation with default parameters (left), GP approximation with learned parameters (middle), and original function to be approximated (right)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
